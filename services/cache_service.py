#!/usr/bin/env python3
"""
ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µãƒ¼ãƒ“ã‚¹
OpenAI APIã¨D-Logicåˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è² è·è»½æ¸›
Redisçµ±åˆç‰ˆ - RedisãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å„ªå…ˆçš„ã«ä½¿ç”¨
"""
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Tuple
import hashlib
import json
from functools import lru_cache
import logging
import os
from pathlib import Path
LOCK_FILE_PATH = Path("/tmp/uma_prewarm.lock")
LOCK_REDIS_KEY = "cache_prewarm_lock:nar_v2"
LOCK_TTL_SECONDS = 1800


def _acquire_prewarm_lock() -> Tuple[bool, bool]:
    """ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒ ã®é‡è¤‡å®Ÿè¡Œã‚’é˜²ããŸã‚ã®ãƒ­ãƒƒã‚¯ã‚’å–å¾—"""
    redis_lock_acquired = False
    try:
        if cache_service.redis_cache and cache_service.redis_cache.is_connected():
            client = cache_service.redis_cache.client
            if client.set(LOCK_REDIS_KEY, os.getpid(), nx=True, ex=LOCK_TTL_SECONDS):
                redis_lock_acquired = True
                return True, True
    except Exception as exc:  # pragma: no cover - safety
        logger.debug("Prewarm redis lock failed: %s", exc)

    try:
        LOCK_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
        fd = os.open(LOCK_FILE_PATH, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
        with os.fdopen(fd, 'w') as handle:
            handle.write(str(os.getpid()))
        return True, redis_lock_acquired
    except FileExistsError:
        return False, redis_lock_acquired
    except Exception as exc:  # pragma: no cover - safety
        logger.debug("Prewarm file lock failed: %s", exc)
        return False, redis_lock_acquired


def _release_prewarm_lock(redis_lock_acquired: bool) -> None:
    """å–å¾—ã—ãŸãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒ ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾"""
    if redis_lock_acquired:
        try:
            if cache_service.redis_cache and cache_service.redis_cache.is_connected():
                cache_service.redis_cache.client.delete(LOCK_REDIS_KEY)
        except Exception as exc:  # pragma: no cover - safety
            logger.debug("Prewarm redis unlock failed: %s", exc)

    try:
        if LOCK_FILE_PATH.exists():
            LOCK_FILE_PATH.unlink()
    except Exception as exc:  # pragma: no cover - safety
        logger.debug("Prewarm file unlock failed: %s", exc)

logger = logging.getLogger(__name__)

# Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    from services.redis_cache import get_redis_cache, RedisCache
    REDIS_AVAILABLE = True
    logger.info("Redis cache module loaded successfully")
except ImportError:
    REDIS_AVAILABLE = False
    logger.warning("Redis cache not available, using memory cache only")

class CacheService:
    """ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.hit_count = 0
        self.miss_count = 0
        
        # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        self.redis_cache: Optional[RedisCache] = None
        if REDIS_AVAILABLE:
            try:
                self.redis_cache = get_redis_cache()
                if self.redis_cache.is_connected():
                    logger.info("Redis cache connected successfully")
                else:
                    logger.warning("Redis cache not connected, using memory cache")
                    self.redis_cache = None
            except Exception as e:
                logger.error(f"Failed to initialize Redis cache: {e}")
                self.redis_cache = None
        
        # TTLè¨­å®šï¼ˆç”¨é€”åˆ¥ï¼‰
        self.ttl_settings = {
            'chat_response': timedelta(hours=48),      # ãƒãƒ£ãƒƒãƒˆå¿œç­”: 48æ™‚é–“ï¼ˆå¢—åŠ ï¼‰
            'dlogic_analysis': timedelta(hours=72),    # D-Logicåˆ†æ: 72æ™‚é–“ï¼ˆå¢—åŠ ï¼‰
            'imlogic_analysis': timedelta(hours=72),   # IMLogicåˆ†æ: 72æ™‚é–“
            'ilogic_analysis': timedelta(hours=48),    # I-Logicåˆ†æ: 48æ™‚é–“
            'flogic_analysis': timedelta(hours=48),    # F-Logicåˆ†æ: 48æ™‚é–“
            'metalogic_analysis': timedelta(hours=48), # MetaLogicåˆ†æ: 48æ™‚é–“
            'weather_analysis': timedelta(hours=24),   # å¤©å€™é©æ€§: 24æ™‚é–“ï¼ˆå¢—åŠ ï¼‰
            'faq_response': timedelta(days=14),        # FAQ: 14æ—¥é–“ï¼ˆå¢—åŠ ï¼‰
            'race_analysis': timedelta(hours=12),      # ãƒ¬ãƒ¼ã‚¹åˆ†æ: 12æ™‚é–“ï¼ˆå¢—åŠ ï¼‰
            'horse_data': timedelta(days=7),           # é¦¬ãƒ‡ãƒ¼ã‚¿: 7æ—¥é–“
            'jockey_data': timedelta(days=7),          # é¨æ‰‹ãƒ‡ãƒ¼ã‚¿: 7æ—¥é–“
            'viewlogic_flow': timedelta(hours=6),      # ViewLogicå±•é–‹äºˆæƒ³
            'viewlogic_trend': timedelta(hours=6),     # ViewLogicå‚¾å‘åˆ†æ
            'viewlogic_recommendation': timedelta(hours=6),  # ViewLogicæ¨å¥¨
            'viewlogic_history': timedelta(hours=12),  # ViewLogicéå»ãƒ‡ãƒ¼ã‚¿
            'viewlogic_sire': timedelta(hours=24),     # ViewLogicè¡€çµ±åˆ†æ
        }
    
    def _generate_key(self, prefix: str, data: Any) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆæ­£è¦åŒ–ä»˜ãï¼‰"""
        # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        if isinstance(data, dict):
            # è¾æ›¸ã®å€¤ã‚’æ­£è¦åŒ–
            normalized_data = {}
            for k, v in data.items():
                # æ­£è¦åŒ–ã‚’å‰Šé™¤ã—ã€ãã®ã¾ã¾ä½¿ç”¨
                normalized_data[k] = v
            data_str = json.dumps(normalized_data, sort_keys=True, ensure_ascii=False)
        elif isinstance(data, list):
            # ãƒªã‚¹ãƒˆã®å ´åˆã¯ãã®ã¾ã¾ã‚½ãƒ¼ãƒˆ
            data_str = json.dumps(sorted(data) if all(isinstance(x, str) for x in data) else data, ensure_ascii=False)
        else:
            # æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            data_str = str(data)
        
        # MD5ãƒãƒƒã‚·ãƒ¥ã§ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        hash_obj = hashlib.md5(data_str.encode('utf-8'))
        return f"{prefix}:{hash_obj.hexdigest()}"
    
    def get(self, prefix: str, data: Any) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆRediså„ªå…ˆï¼‰"""
        key = self._generate_key(prefix, data)
        
        # Redisã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        if self.redis_cache and self.redis_cache.is_connected():
            try:
                redis_key = f"dlogic:{key}"
                value = self.redis_cache.get(redis_key)
                if value is not None:
                    self.hit_count += 1
                    hit_rate = self.get_hit_rate()
                    logger.info(
                        "Cache hit [redis] prefix=%s hit_rate=%.1f%% key=%s",
                        prefix,
                        hit_rate,
                        redis_key
                    )
                    return value
            except Exception as e:
                logger.warning(f"Redis get failed: {e}, falling back to memory cache")
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
        if key in self.cache:
            entry = self.cache[key]
            # æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯
            if datetime.now() < entry['expires_at']:
                self.hit_count += 1
                hit_rate = self.get_hit_rate()
                logger.info(
                    "Cache hit [memory] prefix=%s hit_rate=%.1f%% key=%s",
                    prefix,
                    hit_rate,
                    key
                )
                return entry['value']
            else:
                # æœŸé™åˆ‡ã‚Œã¯å‰Šé™¤
                del self.cache[key]
        
        self.miss_count += 1
        logger.info(
            "Cache miss prefix=%s hit_rate=%.1f%% key=%s",
            prefix,
            self.get_hit_rate(),
            key
        )
        return None
    
    def set(self, prefix: str, data: Any, value: Any, ttl_override: Optional[timedelta] = None) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆRediså„ªå…ˆï¼‰"""
        key = self._generate_key(prefix, data)
        
        # TTLæ±ºå®š
        ttl = ttl_override or self.ttl_settings.get(prefix, timedelta(hours=24))
        
        # Redisã«ä¿å­˜ã‚’è©¦ã¿ã‚‹
        if self.redis_cache and self.redis_cache.is_connected():
            try:
                redis_key = f"dlogic:{key}"
                ttl_seconds = int(ttl.total_seconds())
                success = self.redis_cache.set(redis_key, value, ttl=ttl_seconds)
                if success:
                    logger.debug(f"Saved to Redis cache: {redis_key}")
            except Exception as e:
                logger.warning(f"Redis set failed: {e}, saving to memory cache")
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        self.cache[key] = {
            'value': value,
            'created_at': datetime.now(),
            'expires_at': datetime.now() + ttl,
            'prefix': prefix
        }
        
        # ãƒ¡ãƒ¢ãƒªç®¡ç†ï¼ˆæœ€å¤§1000ã‚¨ãƒ³ãƒˆãƒªï¼‰
        if len(self.cache) > 1000:
            self._cleanup_old_entries()
    
    def _cleanup_old_entries(self):
        """å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
        now = datetime.now()
        # æœŸé™åˆ‡ã‚Œã‚’å‰Šé™¤
        expired_keys = [k for k, v in self.cache.items() if v['expires_at'] < now]
        for key in expired_keys:
            del self.cache[key]
        
        # ãã‚Œã§ã‚‚å¤šã„å ´åˆã¯å¤ã„é †ã«å‰Šé™¤
        if len(self.cache) > 800:
            sorted_items = sorted(
                self.cache.items(),
                key=lambda x: x[1]['created_at']
            )
            for key, _ in sorted_items[:200]:
                del self.cache[key]
    
    def clear_prefix(self, prefix: str):
        """ç‰¹å®šã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        keys_to_delete = [k for k, v in self.cache.items() if v.get('prefix') == prefix]
        for key in keys_to_delete:
            del self.cache[key]
        print(f"ğŸ—‘ï¸ {prefix}ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢: {len(keys_to_delete)}ä»¶")
    
    def get_hit_rate(self) -> float:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’å–å¾—"""
        total = self.hit_count + self.miss_count
        if total == 0:
            return 0.0
        return (self.hit_count / total) * 100
    
    def get_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {
            'total_entries': len(self.cache),
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': self.get_hit_rate(),
            'memory_usage_mb': self._estimate_memory_usage(),
            'entries_by_prefix': {}
        }
        
        # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹åˆ¥ã®çµ±è¨ˆ
        for key, entry in self.cache.items():
            prefix = entry.get('prefix', 'unknown')
            if prefix not in stats['entries_by_prefix']:
                stats['entries_by_prefix'][prefix] = 0
            stats['entries_by_prefix'][prefix] += 1
        
        return stats
    
    def _estimate_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®šï¼ˆMBï¼‰"""
        # ç°¡æ˜“çš„ãªæ¨å®š
        total_size = 0
        for key, entry in self.cache.items():
            # ã‚­ãƒ¼ã®ã‚µã‚¤ã‚º
            total_size += len(key.encode('utf-8'))
            # å€¤ã®ã‚µã‚¤ã‚ºï¼ˆJSONåŒ–ã—ã¦æ¨å®šï¼‰
            try:
                value_str = json.dumps(entry, ensure_ascii=False)
                total_size += len(value_str.encode('utf-8'))
            except:
                total_size += 1000  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1KBã¨ä»®å®š
        
        return total_size / (1024 * 1024)  # MBå¤‰æ›


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå…¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å…±æœ‰ï¼‰
def prewarm_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ï¼ˆG1ãƒ¬ãƒ¼ã‚¹ç”¨ï¼‰"""
    logger.info("Starting cache prewarming for G1 races...")
    lock_acquired, redis_lock_acquired = _acquire_prewarm_lock()
    if not lock_acquired:
        logger.info("Cache prewarming skipped: another worker already running prewarm.")
        return 0

    warmed = 0
    try:
        # G1ãƒ¬ãƒ¼ã‚¹ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹é¦¬åãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰
        popular_horses = [
            "ã‚¤ã‚¯ã‚¤ãƒãƒƒã‚¯ã‚¹", "ãƒ‰ã‚¦ãƒ‡ãƒ¥ãƒ¼ã‚¹", "ãƒªãƒãƒ†ã‚£ã‚¢ã‚¤ãƒ©ãƒ³ãƒ‰",
            "ã‚½ãƒ€ã‚·", "ã‚¸ã‚ªã‚°ãƒªãƒ•", "ã‚¹ã‚¿ãƒ¼ã‚ºã‚ªãƒ³ã‚¢ãƒ¼ã‚¹"
        ]

        # ä¸»è¦ç«¶é¦¬å ´
        major_venues = ["æ±äº¬", "ä¸­å±±", "äº¬éƒ½", "é˜ªç¥"]

        try:
            from services.fast_dlogic_engine import FastDLogicEngine
            engine = FastDLogicEngine()

            for horse_name in popular_horses:
                for venue in major_venues:
                    cache_data = {
                        'horse_name': horse_name,
                        'venue': venue,
                        'analysis_type': 'dlogic',
                        'region': 'jra'
                    }

                    key = cache_service._generate_key('dlogic_analysis', cache_data)

                    if cache_service.redis_cache and cache_service.redis_cache.is_connected():
                        redis_key = f"dlogic:{key}"
                        if cache_service.redis_cache.exists(redis_key):
                            continue

                    try:
                        result = engine.analyze_single_horse(horse_name)
                        cache_service.set(
                            'dlogic_analysis',
                            cache_data,
                            result,
                            ttl_override=timedelta(days=3)
                        )
                        warmed += 1
                        logger.debug(f"Prewarmed JRA cache for {horse_name} at {venue}")
                    except Exception as e:
                        logger.warning(f"Failed to prewarm JRA horse {horse_name}: {e}")

        except Exception as e:
            logger.error(f"Cache prewarming failed for JRA: {e}")

        # åœ°æ–¹ç«¶é¦¬(NAR)å‘ã‘ã®ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒ 
        try:
            from services.local_fast_dlogic_engine_v2 import LocalFastDLogicEngineV2
            local_engine = LocalFastDLogicEngineV2()
            local_manager = local_engine.raw_manager

            local_horses = []
            if hasattr(local_manager, 'get_sample_horses'):
                local_horses = local_manager.get_sample_horses(limit=8)
            elif hasattr(local_manager, 'get_all_horse_names'):
                local_horses = local_manager.get_all_horse_names()[:8]

            local_horses = (local_horses or [])[:8]
            local_venues = ["å¤§äº•", "å·å´", "èˆ¹æ©‹"]
            local_distances = [1200, 1600, 2000]

            logger.info(
                "NAR D-Logic prewarm coverage: horses=%d venues=%d distances=%d",
                len(local_horses),
                len(local_venues),
                len(local_distances)
            )

            for horse_name in local_horses:
                try:
                    local_manager.calculate_dlogic_realtime(horse_name)
                except Exception as e:
                    logger.debug(f"Shard warm-up failed for {horse_name}: {e}")
                    continue

                for venue in local_venues:
                    for distance in local_distances:
                        cache_data = {
                            'horse_name': horse_name,
                            'venue': venue,
                            'distance': distance,
                            'analysis_type': 'dlogic',
                            'region': 'nar'
                        }

                        key = cache_service._generate_key('dlogic_analysis', cache_data)
                        if cache_service.redis_cache and cache_service.redis_cache.is_connected():
                            redis_key = f"dlogic:{key}"
                            if cache_service.redis_cache.exists(redis_key):
                                continue

                        try:
                            result = local_manager.calculate_dlogic_realtime(horse_name)
                            cache_service.set(
                                'dlogic_analysis',
                                cache_data,
                                result,
                                ttl_override=timedelta(days=2)
                            )
                            warmed += 1
                            logger.debug(f"Prewarmed NAR cache for {horse_name} at {venue} {distance}m")
                        except Exception as e:
                            logger.warning(f"Failed to prewarm NAR horse {horse_name} at {venue} {distance}m: {e}")

        except Exception as e:
            logger.error(f"Cache prewarming failed for NAR: {e}")

        logger.info(f"Cache prewarming completed. Warmed {warmed} entries.")
    finally:
        _release_prewarm_lock(redis_lock_acquired)

    return warmed


def schedule_cache_prewarm():
    """å®šæœŸçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
    import threading
    import time
    
    def prewarm_worker():
        while True:
            try:
                # æ¯æœ4æ™‚ã«ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
                now = datetime.now()
                next_run = now.replace(hour=4, minute=0, second=0, microsecond=0)
                if next_run < now:
                    next_run += timedelta(days=1)
                
                wait_seconds = (next_run - now).total_seconds()
                logger.info(f"Next cache prewarm scheduled in {wait_seconds/3600:.1f} hours")
                time.sleep(wait_seconds)
                
                # ãƒ—ãƒªã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
                prewarm_cache()
                
            except Exception as e:
                logger.error(f"Prewarm scheduler error: {e}")
                time.sleep(3600)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1æ™‚é–“å¾Œã«å†è©¦è¡Œ
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
    thread = threading.Thread(target=prewarm_worker, daemon=True)
    thread.start()
    logger.info("Cache prewarm scheduler started")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå…¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å…±æœ‰ï¼‰
cache_service = CacheService()


# ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿é–¢æ•°
def cached(prefix: str, ttl: Optional[timedelta] = None):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”¨ã®ãƒ‡ãƒ¼ã‚¿
            cache_data = {
                'args': args,
                'kwargs': kwargs
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cached_value = cache_service.get(prefix, cache_data)
            if cached_value is not None:
                return cached_value
            
            # å®Ÿè¡Œã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            result = func(*args, **kwargs)
            cache_service.set(prefix, cache_data, result, ttl)
            return result
        
        return wrapper
    return decorator